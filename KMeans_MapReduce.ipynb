{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_n2XmHHGVNfZ"
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "import random\n",
        "import math\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from sklearn.metrics import adjusted_rand_score, calinski_harabasz_score\n",
        "import statistics\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File location and type\n",
        "file_glass = \"/content/glass.csv\"\n",
        "file_iris = \"/content/iris.csv\"\n",
        "file_parkinsons = \"/content/parkinsons.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_iris)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAZoyuEdVOxi",
        "outputId": "2b9944fe-b87b-43ee-92bf-b399de3dbf30"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+---+-----+\n",
            "| f1| f2| f3| f4|class|\n",
            "+---+---+---+---+-----+\n",
            "|5.1|3.5|1.4|0.2|    0|\n",
            "|4.9|3.0|1.4|0.2|    0|\n",
            "|4.7|3.2|1.3|0.2|    0|\n",
            "|4.6|3.1|1.5|0.2|    0|\n",
            "|5.0|3.6|1.4|0.2|    0|\n",
            "|5.4|3.9|1.7|0.4|    0|\n",
            "|4.6|3.4|1.4|0.3|    0|\n",
            "|5.0|3.4|1.5|0.2|    0|\n",
            "|4.4|2.9|1.4|0.2|    0|\n",
            "|4.9|3.1|1.5|0.1|    0|\n",
            "|5.4|3.7|1.5|0.2|    0|\n",
            "|4.8|3.4|1.6|0.2|    0|\n",
            "|4.8|3.0|1.4|0.1|    0|\n",
            "|4.3|3.0|1.1|0.1|    0|\n",
            "|5.8|4.0|1.2|0.2|    0|\n",
            "|5.7|4.4|1.5|0.4|    0|\n",
            "|5.4|3.9|1.3|0.4|    0|\n",
            "|5.1|3.5|1.4|0.3|    0|\n",
            "|5.7|3.8|1.7|0.3|    0|\n",
            "|5.1|3.8|1.5|0.3|    0|\n",
            "+---+---+---+---+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_to_df(datapath):\n",
        "    file_location = datapath\n",
        "    file_type = \"csv\"\n",
        "\n",
        "    # CSV options\n",
        "    infer_schema = \"true\"\n",
        "    first_row_is_header = \"true\"\n",
        "    delimiter = \",\"\n",
        "\n",
        "    # The applied options are for CSV files. For other file types, these will be ignored.\n",
        "    df = spark.read.format(file_type) \\\n",
        "      .option(\"inferSchema\", infer_schema) \\\n",
        "      .option(\"header\", first_row_is_header) \\\n",
        "      .option(\"sep\", delimiter) \\\n",
        "      .load(file_location)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "TMWycDHgYS05"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_nearest_centroid(centroids, point):\n",
        "\n",
        "    min_dist = float(\"inf\")\n",
        "\n",
        "    centro_id = 0\n",
        "    for cid in range(len(centroids)):\n",
        "        euc_dist =0\n",
        "        for  i in range(len(centroids[cid])):\n",
        "            euc_dist+= (point[i]-centroids[cid][i])**2\n",
        "        distance  = math.sqrt(euc_dist)\n",
        "        if distance < min_dist:\n",
        "            min_dist = distance\n",
        "            centroid_id = cid\n",
        "    return (centroid_id,point)"
      ],
      "metadata": {
        "id": "XWBzsCo6YV2M"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convergence_check(new, curr,threshold):\n",
        "    num_centroids = len(new)\n",
        "    dimensions = len(new[0])\n",
        "    for i in range(num_centroids):\n",
        "        temp_dist = 0\n",
        "        for j in range(dimensions):\n",
        "            temp_dist += (new[i][j] - curr[i][j])**2\n",
        "        temp_dist = math.sqrt(temp_dist)\n",
        "        if temp_dist > threshold:\n",
        "            return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "4zyXB2mKYXc-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans_map_reduce(dataset, kclusters, ct = 0.0001, maxiter = 30, n = 10):\n",
        "    data = csv_to_df(dataset)\n",
        "\n",
        "    data_features = data.select(data.columns[:-1])\n",
        "    data_class = data.select('class')\n",
        "\n",
        "    # Normalize data using MinMaxScaler\n",
        "    assembler = VectorAssembler(inputCols=data_features.columns, outputCol=\"features\")\n",
        "    feature_vector = assembler.transform(data_features)\n",
        "\n",
        "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "    scaler_model = scaler.fit(feature_vector)\n",
        "    scaled_data = scaler_model.transform(feature_vector)\n",
        "\n",
        "    normalized_features_rdd = scaled_data.select(\"scaled_features\").rdd.map(lambda x: x[0] )\n",
        "    normalized_data_tup = normalized_features_rdd.map(lambda p: tuple(p))\n",
        "\n",
        "\n",
        "    centroids = normalized_data_tup.takeSample(True, kclusters)\n",
        "\n",
        "    for iteration in range(maxiter):\n",
        "\n",
        "        mapped_points = normalized_data_tup.map(lambda point: find_nearest_centroid(centroids, point))\n",
        "        groupby_key = mapped_points.map(lambda point: (point[0], (point[1],1)))\n",
        "        reduceby_key = groupby_key.reduceByKey(lambda x,y: (tuple(x[0][i]+y[0][i] for i in range(len(x[0]))), x[1]+y[1]))\n",
        "        new_centroids = reduceby_key.mapValues(lambda x: tuple(x[0][i]/x[1] for i in range(len(x[0]))))\n",
        "        new_centroids = new_centroids.sortByKey().collect()\n",
        "        new_centroids = [c[1] for c in new_centroids]\n",
        "\n",
        "        og_labels = [row['class'] for row in data_class.collect()]\n",
        "        predicted_labels = [item[0] for item in mapped_points.collect()]\n",
        "\n",
        "        data_features_ch = [list(row) for row in normalized_data_tup.collect()]\n",
        "\n",
        "        ch_score = calinski_harabasz_score(data_features_ch, predicted_labels)\n",
        "        rand_score = adjusted_rand_score(og_labels, predicted_labels)\n",
        "\n",
        "\n",
        "        if (convergence_check(new_centroids, centroids, ct)):\n",
        "            return ch_score, rand_score\n",
        "        else:\n",
        "            centroids = new_centroids\n",
        "\n",
        "    return ch_score, rand_score\n",
        "\n",
        "\n",
        "\n",
        "print(kmeans_map_reduce(file_iris, 7, maxiter = 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leEnNMZyYZnU",
        "outputId": "bafd8915-e3f8-4cf4-f702-4b103a19bfea"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(225.53552597341655, 0.436715767759047)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans_k_many_experiments(dataset, ct = 0.0001, maxiter = 30, n = 10):\n",
        "    ret = []\n",
        "    k_clusters = [2,3,4,5,6]\n",
        "    for k in k_clusters:\n",
        "        ch_scores = []\n",
        "        rand_scores = []\n",
        "        for i in range(n):\n",
        "            scores = kmeans_map_reduce(dataset, k, ct, maxiter)\n",
        "            ch_scores.append(scores[0])\n",
        "            rand_scores.append(scores[1])\n",
        "        avg_ch = statistics.mean(ch_scores)\n",
        "        std_ch = statistics.stdev(ch_scores)\n",
        "        avg_rand = statistics.mean(rand_scores)\n",
        "        std_rand = statistics.stdev(rand_scores)\n",
        "        ret.append((k, (avg_ch, std_ch), (avg_rand, std_rand)))\n",
        "    print(ret)\n",
        "\n",
        "\n",
        "\n",
        "kmeans_k_many_experiments(file_iris, n=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkm-LyZ2YiUj",
        "outputId": "01fdc09c-13aa-4ee2-afbb-a9ef10b8bfc7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(2, (353.36740323251195, 0.0), (0.5681159420289855, 0.0)), (3, (327.59160689878, 65.30232030289393), (0.6588639134918161, 0.12117468350111307)), (4, (298.81355614031577, 31.720556329328307), (0.6313195353060308, 0.04680681104323954)), (5, (250.74076590366252, 21.784278287214967), (0.5940452163833395, 0.059477874694107466)), (6, (238.4158212060635, 34.90048154343617), (0.522023796160944, 0.07357835865887383))]\n"
          ]
        }
      ]
    }
  ]
}